{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GNN_hands-on.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNhk/4LpLP8MlyNScLyK66K"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kImg_1n_UUeM","colab_type":"text"},"source":["# Get started"]},{"cell_type":"markdown","metadata":{"id":"h2yE36slUQku","colab_type":"text"},"source":["Node Classification"]},{"cell_type":"code","metadata":{"id":"EJkkd9QOUENp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593539281653,"user_tz":-540,"elapsed":1019,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r0VKPYtMZEgF","colab_type":"text"},"source":["DGL(Deep Graph Library)은 backend로 pytorch tensorflow mxnet을 사용할 수 있다.([참조](https://docs.dgl.ai/en/0.4.x/install/backend.html) )"]},{"cell_type":"code","metadata":{"id":"bBvNeMc8UJJ4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1593539285342,"user_tz":-540,"elapsed":4348,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}},"outputId":"60278982-cef9-4bba-dd03-ce479b9b504d"},"source":["!pip install dgl\n","import dgl\n","from dgl import DGLGraph\n","\n","dgl.load_backend('pytorch')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: dgl in /usr/local/lib/python3.6/dist-packages (0.4.3.post2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.18.5)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.4.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.23.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (2020.6.20)\n"],"name":"stdout"},{"output_type":"stream","text":["Using backend: pytorch\n","Using backend: pytorch\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"y9LMqf6AU-VU","colab_type":"text"},"source":["# GNN 모델"]},{"cell_type":"markdown","metadata":{"id":"C8uYZpjvVDOX","colab_type":"text"},"source":["Node representation.\n","DGL의 사용방법 2가지:\n","* nn.Module\n","* message passing방법"]},{"cell_type":"markdown","metadata":{"id":"80r1-xYLVWbR","colab_type":"text"},"source":["GraphSage : \n","$$h_{N(v)}^{(l)} \\gets AGGREGATE_k({h_u^{(l-1)}, \\forall u \\in N(v)})$$\n","$$h_v^{(l)} \\gets \\sigma(W^k \\cdot CONCAT(h_v^{(l-1)}, h_{N(v)}^{(l)})),$$\n","\n","where $N(v)$ is the neighborhood of node $v$ and $l$ is the layer Id.\n","$N(v)$ 는 노드 $v$의 이웃. $l$는 레이어 번호(Id)."]},{"cell_type":"markdown","metadata":{"id":"oBsKoRkZVqLq","colab_type":"text"},"source":["GraphSage는 multi-Layer로 구성된다. 각 레이어에서 노드는 edge로 이어진 이웃 노드에 접근한다. 레이어가 k개이면, k와 k보다 작은 수 만큼 떨어진 node와의 관계를 본다. k만큼 멀리있는 노드를 k-hop이라고 한다. GraphSage의 output은 각 node의 representation이다.\n","\n","<img src=\"https://github.com/zheng-da/DGL_devday_tutorial/raw/master/GNN.png\" alt=\"drawing\" width=\"600\"/>"]},{"cell_type":"markdown","metadata":{"id":"EWfn4CKtWZQr","colab_type":"text"},"source":["여기에선, DGL의 `nn` 모듈을 사용한다. `SAGEConv`가 `GraphSage`의 각 레이어에 들어가있다."]},{"cell_type":"code","metadata":{"id":"O-aNxtoEU9sB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593539285343,"user_tz":-540,"elapsed":3351,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}}},"source":["from dgl.nn.pytorch import conv as dgl_conv\n","\n","class GraphSAGEModel(nn.Module):\n","    def __init__(self,\n","                 in_feats,\n","                 n_hidden,\n","                 out_dim,\n","                 n_layers,\n","                 activation,\n","                 dropout,\n","                 aggregator_type):\n","        super(GraphSAGEModel, self).__init__()\n","        self.layers = nn.ModuleList()\n","\n","        # input layer\n","        self.layers.append(dgl_conv.SAGEConv(in_feats, n_hidden, aggregator_type,\n","                                         feat_drop=dropout, activation=activation))\n","        # hidden layers\n","        for i in range(n_layers - 1):\n","            self.layers.append(dgl_conv.SAGEConv(n_hidden, n_hidden, aggregator_type,\n","                                             feat_drop=dropout, activation=activation))\n","        # output layer\n","        self.layers.append(dgl_conv.SAGEConv(n_hidden, out_dim, aggregator_type,\n","                                         feat_drop=dropout, activation=None))\n","\n","    def forward(self, g, features):\n","        h = features\n","        for layer in self.layers:\n","            h = layer(g, h)\n","        return h"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0IFnIOlXNai","colab_type":"text"},"source":["다양한 `GraphConv`의 예제.([참조](https://docs.dgl.ai/en/0.4.x/tutorials/models/))"]},{"cell_type":"markdown","metadata":{"id":"6cpMqz0wXW84","colab_type":"text"},"source":["# 데이터셋"]},{"cell_type":"markdown","metadata":{"id":"8MdnQOsEXaYb","colab_type":"text"},"source":["DGL안에 데이터셋이 많다. [참조](https://doc.dgl.ai/api/python/data.html#dataset-classes)"]},{"cell_type":"markdown","metadata":{"id":"ED__XHiRX71Q","colab_type":"text"},"source":["여기에선 Pubmed라는 논문인용 네트워크 데이터넷을 사용한다.\n","\n","노드는 각 paper이고, edge는 인용된 두 논문을 이어준다. 19,717 논문(node)들과 88,651 edge가 있다. 각 node는 feature와 class를 가지고 있다(sparse bag-of-words)."]},{"cell_type":"code","metadata":{"id":"oCukGJa7WYh3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151},"executionInfo":{"status":"ok","timestamp":1593539286690,"user_tz":-540,"elapsed":3404,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}},"outputId":"87b4dec6-958d-4d9b-bf28-8e3eed04a24b"},"source":["from dgl.data import citegrh\n","\n","data = citegrh.load_pubmed()\n","\n","features = torch.FloatTensor(data.features)     # feature를 넘파이에서 파이토치 텐서로 변환\n","in_feats = features.shape[1]                    # dimension = 500\n","labels = torch.LongTensor(data.labels)          # 파이토치 텐서로 변환\n","n_classes = data.num_labels                     # 클래스 갯수"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Finished data loading and preprocessing.\n","  NumNodes: 19717\n","  NumEdges: 88651\n","  NumFeats: 500\n","  NumClasses: 3\n","  NumTrainingSamples: 60\n","  NumValidationSamples: 500\n","  NumTestSamples: 1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L57UQsTtY34M","colab_type":"text"},"source":["네트워크(graph) 구조를 [NetworkX](https://networkx.github.io) 객체를 쓴다."]},{"cell_type":"code","metadata":{"id":"Wd4aa_yvYjyo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593539286691,"user_tz":-540,"elapsed":3011,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}}},"source":["import networkx as nx\n","data.graph.remove_edges_from(nx.selfloop_edges(data.graph)) #self-loop제거"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-TwIuC_Y7XG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593539286693,"user_tz":-540,"elapsed":1994,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}}},"source":["g = DGLGraph(data.graph)    #DGL그래프로 변환\n","g.readonly()                #오로지 더 빠르게 읽기 위해. 수정이 없는경우 사용. (메모리 절약)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q7cmLxtSawIz","colab_type":"text"},"source":["# Semi-supervised node classification"]},{"cell_type":"markdown","metadata":{"id":"a0DHhM_ja73S","colab_type":"text"},"source":["모든 graph를 가지고 있으며, 각 node의 feature를 가지고 있고, 몇개만 class label이 있으며, 몇개는 없다. class label이 없는 node의 class를 예측한다. labeled node와 unlabeled node를 같이 학습한다.\n","\n","<img src=\"https://github.com/zheng-da/DGL_devday_tutorial/raw/master/node_classify1.png\" alt=\"drawing\" width=\"200\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"9fGOB09gbZ5S","colab_type":"text"},"source":["2층 GraphSage"]},{"cell_type":"code","metadata":{"id":"DeHiPcGVat-T","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593539287208,"user_tz":-540,"elapsed":913,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}}},"source":["n_hidden = 64\n","n_layers = 2\n","dropout = 0.5\n","aggregator_type = 'gcn'\n","\n","gconv_model = GraphSAGEModel(in_feats,\n","                             n_hidden,\n","                             n_classes,\n","                             n_layers,\n","                             F.relu,\n","                             dropout,\n","                             aggregator_type)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ts0qbX49blRk","colab_type":"text"},"source":["`GraphSage`는 DGLGraph객체와 node의 feature들을 input으로 받는다. output은 node embeddings이다. node embeddings로 cross entropy loss를 써서 훈련한다.\n","\n"]},{"cell_type":"code","metadata":{"id":"pvV1IlOjbdbI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593539287887,"user_tz":-540,"elapsed":856,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}}},"source":["class NodeClassification(nn.Module):\n","    def __init__(self, gconv_model, n_hidden, n_classes):\n","        super(NodeClassification, self).__init__()\n","        self.gconv_model = gconv_model\n","        self.loss_fcn = torch.nn.CrossEntropyLoss()\n","\n","    def forward(self, g, features, train_mask):\n","        logits = self.gconv_model(g, features)\n","        return self.loss_fcn(logits[train_mask], labels[train_mask])"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJdD4IMucJfb","colab_type":"text"},"source":["Test node에서 classification정확도 함수"]},{"cell_type":"code","metadata":{"id":"BcQMpPUCcIxE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593539288747,"user_tz":-540,"elapsed":602,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}}},"source":["def NCEvaluate(model, g, features, labels, test_mask):\n","    model.eval()\n","    with torch.no_grad():\n","        logits = model.gconv_model(g, features)\n","        logits = logits[test_mask]                          #Test set에서만 accuracy 계산\n","        test_labels = labels[test_mask]\n","        _, indices = torch.max(logits, dim=1)\n","        correct = torch.sum(indices == test_labels)\n","        return correct.item() * 1.0 / len(test_labels)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"heCfJl-_cS4N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"status":"ok","timestamp":1593539951452,"user_tz":-540,"elapsed":973,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}},"outputId":"3c42b06f-9ad4-4391-e22f-5b3f23348453"},"source":["# train/val/text mask 생성\n","train_mask = torch.BoolTensor(data.train_mask)\n","val_mask = torch.BoolTensor(data.val_mask)\n","test_mask = torch.BoolTensor(data.test_mask)\n","\n","print(\"\"\"----Data statistics------'\n","      #Classes %d\n","      #Train samples %d\n","      #Val samples %d\n","      #Test samples %d\"\"\" %\n","          (n_classes,\n","           data.train_mask.sum().item(),\n","           data.val_mask.sum().item(),\n","           data.test_mask.sum().item()))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["----Data statistics------'\n","      #Classes 3\n","      #Train samples 60\n","      #Val samples 500\n","      #Test samples 1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gd-gvBQ-ccsb","colab_type":"text"},"source":["훈련"]},{"cell_type":"code","metadata":{"id":"aXqhay_pcZJs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593540151992,"user_tz":-540,"elapsed":199230,"user":{"displayName":"Heonjin Ha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTgOs11FypxuWukwbr1C8XGxrHDJQqOvIdtDbb=s64","userId":"00011183272248708590"}},"outputId":"7493c1f9-390a-4893-86be-fe8e1a8dfaa6"},"source":["model = NodeClassification(gconv_model, n_hidden, n_classes)\n","\n","# 하이퍼파라미터\n","weight_decay = 5e-4\n","n_epochs = 150\n","lr = 1e-3\n","\n","# 옵티마이저\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","dur = []\n","for epoch in range(n_epochs):\n","    model.train()\n","    \n","    loss = model(g, features, train_mask)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    acc = NCEvaluate(model, g, features, labels, val_mask)\n","    print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f}\"\n","          .format(epoch, loss.item(), acc))\n","\n","acc = NCEvaluate(model, g, features, labels, test_mask)\n","print(\"\\nTest Accuracy {:.4f}\".format(acc))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Epoch 00000 | Loss 1.0792 | Accuracy 0.3880\n","Epoch 00001 | Loss 1.0760 | Accuracy 0.3880\n","Epoch 00002 | Loss 1.0748 | Accuracy 0.3880\n","Epoch 00003 | Loss 1.0666 | Accuracy 0.3880\n","Epoch 00004 | Loss 1.0647 | Accuracy 0.3880\n","Epoch 00005 | Loss 1.0642 | Accuracy 0.3880\n","Epoch 00006 | Loss 1.0599 | Accuracy 0.3880\n","Epoch 00007 | Loss 1.0569 | Accuracy 0.3880\n","Epoch 00008 | Loss 1.0492 | Accuracy 0.3880\n","Epoch 00009 | Loss 1.0500 | Accuracy 0.3880\n","Epoch 00010 | Loss 1.0488 | Accuracy 0.3880\n","Epoch 00011 | Loss 1.0418 | Accuracy 0.3880\n","Epoch 00012 | Loss 1.0437 | Accuracy 0.3880\n","Epoch 00013 | Loss 1.0370 | Accuracy 0.3920\n","Epoch 00014 | Loss 1.0388 | Accuracy 0.4040\n","Epoch 00015 | Loss 1.0383 | Accuracy 0.4220\n","Epoch 00016 | Loss 1.0359 | Accuracy 0.4400\n","Epoch 00017 | Loss 1.0313 | Accuracy 0.4500\n","Epoch 00018 | Loss 1.0329 | Accuracy 0.4600\n","Epoch 00019 | Loss 1.0251 | Accuracy 0.4760\n","Epoch 00020 | Loss 1.0269 | Accuracy 0.4940\n","Epoch 00021 | Loss 1.0240 | Accuracy 0.5040\n","Epoch 00022 | Loss 1.0206 | Accuracy 0.5200\n","Epoch 00023 | Loss 1.0174 | Accuracy 0.5420\n","Epoch 00024 | Loss 1.0185 | Accuracy 0.5620\n","Epoch 00025 | Loss 1.0153 | Accuracy 0.5740\n","Epoch 00026 | Loss 1.0110 | Accuracy 0.5880\n","Epoch 00027 | Loss 1.0110 | Accuracy 0.5900\n","Epoch 00028 | Loss 1.0082 | Accuracy 0.5980\n","Epoch 00029 | Loss 1.0038 | Accuracy 0.6060\n","Epoch 00030 | Loss 1.0040 | Accuracy 0.6100\n","Epoch 00031 | Loss 1.0006 | Accuracy 0.6120\n","Epoch 00032 | Loss 0.9990 | Accuracy 0.6140\n","Epoch 00033 | Loss 1.0010 | Accuracy 0.6200\n","Epoch 00034 | Loss 0.9997 | Accuracy 0.6300\n","Epoch 00035 | Loss 0.9895 | Accuracy 0.6320\n","Epoch 00036 | Loss 0.9902 | Accuracy 0.6420\n","Epoch 00037 | Loss 0.9861 | Accuracy 0.6440\n","Epoch 00038 | Loss 0.9836 | Accuracy 0.6460\n","Epoch 00039 | Loss 0.9825 | Accuracy 0.6480\n","Epoch 00040 | Loss 0.9746 | Accuracy 0.6500\n","Epoch 00041 | Loss 0.9663 | Accuracy 0.6520\n","Epoch 00042 | Loss 0.9688 | Accuracy 0.6500\n","Epoch 00043 | Loss 0.9703 | Accuracy 0.6500\n","Epoch 00044 | Loss 0.9670 | Accuracy 0.6480\n","Epoch 00045 | Loss 0.9568 | Accuracy 0.6480\n","Epoch 00046 | Loss 0.9581 | Accuracy 0.6460\n","Epoch 00047 | Loss 0.9422 | Accuracy 0.6460\n","Epoch 00048 | Loss 0.9442 | Accuracy 0.6480\n","Epoch 00049 | Loss 0.9369 | Accuracy 0.6540\n","Epoch 00050 | Loss 0.9333 | Accuracy 0.6540\n","Epoch 00051 | Loss 0.9311 | Accuracy 0.6540\n","Epoch 00052 | Loss 0.9244 | Accuracy 0.6500\n","Epoch 00053 | Loss 0.9201 | Accuracy 0.6500\n","Epoch 00054 | Loss 0.9149 | Accuracy 0.6500\n","Epoch 00055 | Loss 0.9090 | Accuracy 0.6480\n","Epoch 00056 | Loss 0.9013 | Accuracy 0.6480\n","Epoch 00057 | Loss 0.8933 | Accuracy 0.6520\n","Epoch 00058 | Loss 0.8872 | Accuracy 0.6520\n","Epoch 00059 | Loss 0.8888 | Accuracy 0.6560\n","Epoch 00060 | Loss 0.8811 | Accuracy 0.6640\n","Epoch 00061 | Loss 0.8748 | Accuracy 0.6680\n","Epoch 00062 | Loss 0.8607 | Accuracy 0.6760\n","Epoch 00063 | Loss 0.8641 | Accuracy 0.6800\n","Epoch 00064 | Loss 0.8533 | Accuracy 0.6860\n","Epoch 00065 | Loss 0.8470 | Accuracy 0.6940\n","Epoch 00066 | Loss 0.8449 | Accuracy 0.7060\n","Epoch 00067 | Loss 0.8311 | Accuracy 0.7160\n","Epoch 00068 | Loss 0.8264 | Accuracy 0.7240\n","Epoch 00069 | Loss 0.8173 | Accuracy 0.7320\n","Epoch 00070 | Loss 0.8138 | Accuracy 0.7360\n","Epoch 00071 | Loss 0.8073 | Accuracy 0.7440\n","Epoch 00072 | Loss 0.7969 | Accuracy 0.7540\n","Epoch 00073 | Loss 0.7851 | Accuracy 0.7640\n","Epoch 00074 | Loss 0.7776 | Accuracy 0.7660\n","Epoch 00075 | Loss 0.7773 | Accuracy 0.7660\n","Epoch 00076 | Loss 0.7711 | Accuracy 0.7680\n","Epoch 00077 | Loss 0.7565 | Accuracy 0.7700\n","Epoch 00078 | Loss 0.7449 | Accuracy 0.7740\n","Epoch 00079 | Loss 0.7538 | Accuracy 0.7760\n","Epoch 00080 | Loss 0.7383 | Accuracy 0.7820\n","Epoch 00081 | Loss 0.7309 | Accuracy 0.7840\n","Epoch 00082 | Loss 0.7170 | Accuracy 0.7860\n","Epoch 00083 | Loss 0.7133 | Accuracy 0.7880\n","Epoch 00084 | Loss 0.7020 | Accuracy 0.7880\n","Epoch 00085 | Loss 0.6922 | Accuracy 0.7920\n","Epoch 00086 | Loss 0.6957 | Accuracy 0.7920\n","Epoch 00087 | Loss 0.6872 | Accuracy 0.7900\n","Epoch 00088 | Loss 0.6789 | Accuracy 0.7900\n","Epoch 00089 | Loss 0.6711 | Accuracy 0.7900\n","Epoch 00090 | Loss 0.6578 | Accuracy 0.7940\n","Epoch 00091 | Loss 0.6499 | Accuracy 0.7960\n","Epoch 00092 | Loss 0.6473 | Accuracy 0.8020\n","Epoch 00093 | Loss 0.6439 | Accuracy 0.8040\n","Epoch 00094 | Loss 0.6382 | Accuracy 0.8140\n","Epoch 00095 | Loss 0.6351 | Accuracy 0.8180\n","Epoch 00096 | Loss 0.6156 | Accuracy 0.8200\n","Epoch 00097 | Loss 0.6081 | Accuracy 0.8180\n","Epoch 00098 | Loss 0.6078 | Accuracy 0.8160\n","Epoch 00099 | Loss 0.5960 | Accuracy 0.8160\n","Epoch 00100 | Loss 0.5856 | Accuracy 0.8160\n","Epoch 00101 | Loss 0.5898 | Accuracy 0.8160\n","Epoch 00102 | Loss 0.5880 | Accuracy 0.8160\n","Epoch 00103 | Loss 0.5792 | Accuracy 0.8180\n","Epoch 00104 | Loss 0.5761 | Accuracy 0.8180\n","Epoch 00105 | Loss 0.5764 | Accuracy 0.8200\n","Epoch 00106 | Loss 0.5625 | Accuracy 0.8180\n","Epoch 00107 | Loss 0.5497 | Accuracy 0.8180\n","Epoch 00108 | Loss 0.5447 | Accuracy 0.8180\n","Epoch 00109 | Loss 0.5488 | Accuracy 0.8140\n","Epoch 00110 | Loss 0.5421 | Accuracy 0.8140\n","Epoch 00111 | Loss 0.5502 | Accuracy 0.8160\n","Epoch 00112 | Loss 0.5406 | Accuracy 0.8200\n","Epoch 00113 | Loss 0.5259 | Accuracy 0.8200\n","Epoch 00114 | Loss 0.5266 | Accuracy 0.8200\n","Epoch 00115 | Loss 0.5231 | Accuracy 0.8240\n","Epoch 00116 | Loss 0.5011 | Accuracy 0.8260\n","Epoch 00117 | Loss 0.5051 | Accuracy 0.8300\n","Epoch 00118 | Loss 0.5108 | Accuracy 0.8280\n","Epoch 00119 | Loss 0.5131 | Accuracy 0.8280\n","Epoch 00120 | Loss 0.5189 | Accuracy 0.8260\n","Epoch 00121 | Loss 0.5118 | Accuracy 0.8240\n","Epoch 00122 | Loss 0.4933 | Accuracy 0.8240\n","Epoch 00123 | Loss 0.4943 | Accuracy 0.8280\n","Epoch 00124 | Loss 0.4894 | Accuracy 0.8360\n","Epoch 00125 | Loss 0.4808 | Accuracy 0.8360\n","Epoch 00126 | Loss 0.4871 | Accuracy 0.8440\n","Epoch 00127 | Loss 0.4738 | Accuracy 0.8420\n","Epoch 00128 | Loss 0.4837 | Accuracy 0.8420\n","Epoch 00129 | Loss 0.4726 | Accuracy 0.8400\n","Epoch 00130 | Loss 0.4674 | Accuracy 0.8420\n","Epoch 00131 | Loss 0.4722 | Accuracy 0.8420\n","Epoch 00132 | Loss 0.4527 | Accuracy 0.8460\n","Epoch 00133 | Loss 0.4654 | Accuracy 0.8480\n","Epoch 00134 | Loss 0.4555 | Accuracy 0.8520\n","Epoch 00135 | Loss 0.4718 | Accuracy 0.8560\n","Epoch 00136 | Loss 0.4428 | Accuracy 0.8520\n","Epoch 00137 | Loss 0.4702 | Accuracy 0.8480\n","Epoch 00138 | Loss 0.4593 | Accuracy 0.8460\n","Epoch 00139 | Loss 0.4537 | Accuracy 0.8460\n","Epoch 00140 | Loss 0.4486 | Accuracy 0.8480\n","Epoch 00141 | Loss 0.4474 | Accuracy 0.8500\n","Epoch 00142 | Loss 0.4422 | Accuracy 0.8540\n","Epoch 00143 | Loss 0.4444 | Accuracy 0.8560\n","Epoch 00144 | Loss 0.4376 | Accuracy 0.8560\n","Epoch 00145 | Loss 0.4348 | Accuracy 0.8560\n","Epoch 00146 | Loss 0.4326 | Accuracy 0.8560\n","Epoch 00147 | Loss 0.4129 | Accuracy 0.8560\n","Epoch 00148 | Loss 0.4344 | Accuracy 0.8540\n","Epoch 00149 | Loss 0.4225 | Accuracy 0.8520\n","\n","Test Accuracy 0.8833\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AyBROvV6cehj","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}